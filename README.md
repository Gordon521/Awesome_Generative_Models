# Awesome_Generative_Models
Awesome Generative Models is a curated repository tracking progress in generative AI, serving as a resource for state-of-the-art papers, code, and datasets. *(The project is under active development, and contributions are warmly welcome!)*
# Table of Contents
- [Papers](#papers)
  - [Surveys](#surveys)
  - [Foundation Models](#foundation-models)
  - [Dataset](#datasets)
  - [Tokenizer](#Tokenizer)
  - [Diffusion models](#diffusion-models)
    - [paper list](#paper-list)
  - [Autogressive models](#autogressive-models)
    - [paper list](#paper-list-1)
## Surveys
* A Survey on Generative Diffusion Models. [[Paper]](https://ieeexplore.ieee.org/abstract/document/10419041)
* Personalized Image Generation with Deep Generative Models: A Decade Survey. [[Paper]](https://arxiv.org/abs/2502.13081)
## Foundation Models
- `LDM` **High-Resolution Image Synthesis with Latent Diffusion Models.**
  [[Paper](https://arxiv.org/abs/2112.10752)] [[Code](https://github.com/CompVis/latent-diffusion)]
- `DALL·E 1` **Zero-Shot Text-to-Image Generation.**
  [[Paper](https://arxiv.org/abs/2102.12092)] [[Code](https://github.com/openai/DALL-E)]
- `DALL·E 2` **Hierarchical Text-Conditional Image Generation with CLIP Latents.** 
  [[Paper](https://arxiv.org/abs/2204.06125)] 
- `Imagen` **Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding.**
  [[Paper](https://arxiv.org/abs/2205.11487)]
- `Parti` **Scaling Autoregressive Models for Content-Rich Text-to-Image Generation.**
  [[Paper](https://arxiv.org/abs/2206.10789)]
- `StyleGAN` **A Style-Based Generator Architecture for Generative Adversarial Networks.**
  [[Paper](https://arxiv.org/abs/1812.04948)] [[Code](https://github.com/NVlabs/stylegan)]
- `StyleGAN 2` **Analyzing and Improving the Image Quality of StyleGAN.**
  [[Paper](https://arxiv.org/abs/1912.04958)] [[Code](https://github.com/NVlabs/stylegan2)]
- `StyleGAN 3` **Alias-Free Generative Adversarial Networks.**
  [[Paper](https://arxiv.org/abs/2106.12423)] [[Code](https://github.com/NVlabs/stylegan3)]​
## Dataset
## Tokenizer
- `VAE` **Auto-Encoding Variational Bayes**
  [[Paper](https://arxiv.org/abs/1312.6114)]
- `VQVAE` **Neural Discrete Representation Learning**
  [[Paper](https://arxiv.org/abs/1711.00937)]
- `VQVAE 2` **Generating Diverse High-Fidelity Images with VQ-VAE-2**
  [[Paper](https://arxiv.org/abs/1906.00446)]
- `VAVAE` **Neural Discrete Representation Learning**
  [[Paper](https://arxiv.org/abs/2412.04852)] [[Code](https://github.com/hustvl/LightningDiT)]
- `Titok` **An Image is Worth 32 Tokens for Reconstruction and Generation**
  [[Paper](https://arxiv.org/abs/2406.07550)] [[Code](https://github.com/bytedance/1d-tokenizer)]
- `TA-Titok` **Democratizing Text-to-Image Masked Generative Models with Compact Text-Aware One-Dimensional Tokens**
  [[Paper](https://arxiv.org/abs/2501.07730)] [[Code](https://github.com/bytedance/1d-tokenizer)]
- `MAEtok` **Masked Autoencoders Are Effective Tokenizers for Diffusion Models**
  [[Paper](https://arxiv.org/abs/2502.03444)] [[Code](https://github.com/Hhhhhhao/continuous_tokenizer)]
## Diffusion models
Please also see a LaTeX table (located in xxx) containing a curated list of references from this repository for your convenience in citing directly within your papers.
(To be revised)
- `REPA` **Representation Alignment for Generation: Training Diffusion Transformers Is Easier Than You Think**
  [[Paper](https://arxiv.org/abs/2410.06940)] [[Code](https://github.com/sihyun-yu/REPA)]
- `REG` **Representation Entanglement for Generation: Training Diffusion Transformers Is Much Easier Than You Think**
  [[Paper](https://arxiv.org/abs/2507.01467)] [[Code](https://github.com/Martinser/REG)] [[论文解读](https://zhuanlan.zhihu.com/p/1952346823168595518)]
- `REPA-E` **REPA-E: Unlocking VAE for End-to-End Tuning with Latent Diffusion Transformers**
  [[Paper](https://arxiv.org/abs/2504.10483)] [[Code](https://github.com/End2End-Diffusion/REPA-E)]
- `RAE` **Diffusion Transformers with Representation Autoencoders**
  [[Paper](https://arxiv.org/abs/2510.11690)] [[Code](https://github.com/bytetriper/RAE)]
- `SIT` **SiT: Exploring Flow and Diffusion-based Generative Models with Scalable Interpolant Transformers**
  [[Paper](https://arxiv.org/abs/2401.08740)] [[Code](https://github.com/willisma/SiT)]
- `DIT` **Scalable Diffusion Models with Transformers**
  [[Paper](https://arxiv.org/abs/2212.09748)] [[Code](https://www.github.com/facebookresearch/DiT)]
- `MDT` **Masked Diffusion Transformer is a Strong Image Synthesizer**
  [[Paper](https://arxiv.org/abs/2303.14389)] [[Code](https://github.com/sail-sg/MDT)]
| Methods | Paper | Pub | Params | Resolusion | Steps | gFID | Latency(s) | Throughput(images/s) | Code |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
### Paper list
## Autogressive models
Please also see a LaTeX table (located in xxx) containing a curated list of references from this repository for your convenience in citing directly within your papers.
(To be revised)
| Methods | Paper | Pub | Params | Resolusion | Steps | gFID | Latency(s) | Throughput(images/s) | Code |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
### Paper list
